{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff0a45f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ee1a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    faq_answerable: Annotated[bool, True]\n",
    "    retrieved_faq_docs: Annotated[list, []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b68b057",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def faq_node(state: ChatState):\n",
    "    last_message = state['messages'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e306788e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ANT-PC\\OneDrive\\Desktop\\FinkraftAI\\.venv\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cb2fa",
   "metadata": {},
   "source": [
    "def answer_faq(question: str, faq_data_path: str, threshold: int = 80):\n",
    "    with open(faq_data_path, \"r\") as f:\n",
    "        faq_data=json.load(f)\n",
    "    faq_data=faq_data[\"faqs\"]\n",
    "    for doc in faq_data:\n",
    "        questions = [doc['q']] + doc['q_variants']\n",
    "        score = max(fuzz.partial_ratio(question, q) for q in questions)\n",
    "        if score >= threshold:\n",
    "            return {\n",
    "                \"answer\": doc['a_md'],\n",
    "                \"source\": doc['source']\n",
    "                }\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4aa675",
   "metadata": {},
   "source": [
    "answer_faq(\"SOC compliance\", \"data/faq_data.json\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb552fd",
   "metadata": {},
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e553f2",
   "metadata": {},
   "source": [
    "with open(\"data/knowledge_base.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39648421",
   "metadata": {},
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)\n",
    "chunks = splitter.split_text(data)\n",
    "metadatas = [{\"source\": \"knowledge_base.txt\"} for _ in range(len(chunks))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cfc1f",
   "metadata": {},
   "source": [
    "len(chunks), len(metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917d309",
   "metadata": {},
   "source": [
    "import torch\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def embed_chunks(chunks: list, model_name:str=\"sentence-transformers/all-mpnet-base-v2\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    embedder = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"device\": device})\n",
    "    embeddings = embedder.embed_documents(chunks)   \n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_chunks(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5ca6d",
   "metadata": {},
   "source": [
    "print(f\"\"\"\n",
    "embeddings length: {len(embeddings)}\n",
    "number of chunks: {len(chunks)}\n",
    "number of metadatas: {len(metadatas)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c03994b",
   "metadata": {},
   "source": [
    "len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596a216",
   "metadata": {},
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings_and_text = list(zip(chunks,embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161c7e5",
   "metadata": {},
   "source": [
    "vs = FAISS.from_embeddings(embeddings_and_text, metadatas)\n",
    "vs.save_local(\"faiss_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4279753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vector_store import VectorStore\n",
    "\n",
    "vs = VectorStore(\"faiss_store\")\n",
    "# vs.ingest_data(\"data/knowledge_base.txt\")\n",
    "query = \"What are the api rate limits for the free plan?\"\n",
    "context = vs.vector_search(query, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b43a8f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The API rate limits for the free plan are:\n",
      "\n",
      "* 600 requests per minute per organization\n",
      "* Up to 50 concurrent jobs per workspace\n",
      "\n",
      "This means that you can make a certain number of requests within a given time frame (600 requests per minute) and have up to a certain number of concurrent jobs running at the same time (50 jobs).\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    {query}\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"context\": context, \"query\": query})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e99c9a2",
   "metadata": {},
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c0852",
   "metadata": {},
   "source": [
    "for item in context:\n",
    "    print(item[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6981de1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
